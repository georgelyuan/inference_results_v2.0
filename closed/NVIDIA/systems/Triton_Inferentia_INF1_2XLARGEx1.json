{
    "accelerator_frequency": "",
    "accelerator_host_interconnect": "",
    "accelerator_interconnect": "",
    "accelerator_interconnect_topology": "",
    "accelerator_memory_capacity": "?",
    "accelerator_memory_configuration": "Commodity DRAM",
    "accelerator_model_name": "Inferentia",
    "accelerator_on-chip_memories": "",
    "accelerators_per_node": 1,
    "boot_firmware_version": "",
    "cooling": "",
    "disk_controllers": "",
    "disk_drives": "",
    "division": "closed",
    "filesystem": "",
    "framework": "Pytorch 1.9.1, AWS Neuron 1.9.1.2",
    "host_memory_capacity": "16 GB",
    "host_memory_configuration": "",
    "host_networking": "",
    "host_networking_topology": "",
    "host_processor_caches": "",
    "host_processor_core_count": 8,
    "host_processor_frequency": "",
    "host_processor_interconnect": "",
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz",
    "host_processors_per_node": 1,
    "host_storage_capacity": "500 GB",
    "host_storage_type": "Amazon EBS",
    "hw_notes": "",
    "management_firmware_version": "",
    "network_speed_mbit": "",
    "nics_enabled_connected": "",
    "nics_enabled_firmware": "",
    "nics_enabled_os": "",
    "number_of_nodes": 1,
    "number_of_type_nics_installed": "",
    "operating_system": "Ubuntu 18.04",
    "other_hardware": "",
    "other_software_stack": "Triton 22.01",
    "power_management": "",
    "power_supply_details": "",
    "power_supply_quantity_and_rating_watts": "",
    "status": "available",
    "submitter": "NVIDIA",
    "sw_notes": "Triton Inference Server on AWS Inf1 instance",
    "system_name": "AWS Inf1.2xlarge",
    "system_type": "datacenter"
}
